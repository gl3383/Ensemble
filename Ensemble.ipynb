{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Ensemble- ISLR -Exercise 8.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 8.4.1\n",
    "Draw an example (of your own invention) of a partition of twodimensional feature space that could result from recursive binary\n",
    "splitting. Your example should contain at least six regions. Draw a decision tree corresponding to this partition. Be sure to label all aspects of your figures, including the regions R1,R2, . . ., the cutpoints t1, t2, . . ., and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAdVBMVEUAAABNTU1oAABoaGh8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\nAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "par(xpd = NA)\n",
    "plot(NA, NA, type = \"n\", xlim = c(0,100), ylim = c(0,100), xlab = \"X\", ylab = \"Y\")\n",
    "\n",
    "# t1: x = 50; (50, 0) (50, 100)\n",
    "lines(x = c(50,50), y = c(0,100))\n",
    "text(x = 50, y = 105, labels = c(\"t1\"), col = \"red\")\n",
    "\n",
    "# t2: y = 80; (0, 80) (50, 80)\n",
    "lines(x = c(0,50), y = c(80,80))\n",
    "text(x = -5, y = 80, labels = c(\"t2\"), col = \"red\")\n",
    "\n",
    "# t3: x = 80; (80,0) (80, 100)\n",
    "lines(x = c(80,80), y = c(0,100))\n",
    "text(x = 80, y = 105, labels = c(\"t3\"), col = \"red\")\n",
    "\n",
    "# t4: x = 25; (25,0) (25, 80)\n",
    "lines(x = c(25,25), y = c(0,80))\n",
    "text(x = 25, y = 85, labels = c(\"t4\"), col = \"red\")\n",
    "\n",
    "# t5: y=30; (80,30) (100,30)\n",
    "lines(x = c(80,100), y = c(30,30))\n",
    "text(x = 78, y = 30, labels = c(\"t5\"), col = \"red\")\n",
    "\n",
    "\n",
    "text(x = (50+80)/2, y = 50, labels = c(\"R1\"))\n",
    "text(x = 25, y = (100+80)/2, labels = c(\"R2\"))\n",
    "text(x = (80+100)/2, y = (100+30)/2, labels = c(\"R3\"))\n",
    "text(x = (80+100)/2, y = 30/2, labels = c(\"R4\"))\n",
    "text(x = 37, y = 80/2, labels = c(\"R5\"))\n",
    "text(x = 13, y = 80/2, labels = c(\"R6\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 8.4.2\n",
    "It is mentioned in Section 8.2.3 that boosting using depth-one trees (or stumps) leads to an additive model.Explain why this is the case. You can begin with (8.12) in Algorithm 8.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let f^(x) = 0 and ri = yi  for all values of i.\n",
    "\n",
    "f^1(x) = c1I(x1<t1) + c’1 = 1/λ f1(x1)\n",
    "  \n",
    "Hence f^(x) = λ f^1(x)\n",
    "\t\n",
    "ri  = yi - λ f^1(xi)\n",
    "\n",
    "f^2(x) = c2I(x2<t2) + c’2 = 1/λ f2(x2)\n",
    "\n",
    "Hence f^(x) = λ f^1(x) + λ f^2(x)\n",
    "\n",
    "ri  = yi - λ f^1(xi) - λ f^2(xi)\n",
    "\n",
    "Therefore  f^(x) = ∑^p (subscript)j=1  fj(xj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 8.4.5\n",
    "Suppose we produce ten bootstrapped samples from a data set containing red and green classes. We then apply a classification tree to each bootstrapped sample and, for a specific value of X, produce 10 estimates of P(Class is Red|X):\n",
    "0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, and 0.75.\n",
    "There are two common ways to combine these results together into a single class prediction. One is the majority vote approach discussed in this chapter. The second approach is to classify based on the average probability. In this example, what is the final classification under each of these two approaches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "TRUE"
      ],
      "text/latex": [
       "TRUE"
      ],
      "text/markdown": [
       "TRUE"
      ],
      "text/plain": [
       "[1] TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Majority approach\n",
    "p = c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75)\n",
    "sum(p >= 0.5) > sum(p < 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of red predictions is greater than the number of green predictions based on a 50% threshold, thus RED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.45"
      ],
      "text/latex": [
       "0.45"
      ],
      "text/markdown": [
       "0.45"
      ],
      "text/plain": [
       "[1] 0.45"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Average approach\n",
    "mean(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average of the probabilities is less than the 50% threshold, thus GREEN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 8.4.12\n",
    "Apply boosting, bagging, and random forests to a data set of your choice. Be sure to ﬁt the models on a training set and to evaluate their performance on a test set. How accurate are the results compared to simple methods like linear or logistic regression? Which of these approaches yields the best performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HR = read.csv(file = 'C:/Users/Thiya/Desktop/Wayne/Fall 2017/Analytics course/Project/HR.csv', header = T, sep = ',')\n",
    "sample_size <- floor(0.75 * nrow(HR))\n",
    "training_data <- sample(seq_len(nrow(HR)), size = sample_size)\n",
    "train <- HR[training_data, ]\n",
    "test <- HR[-training_data, ]\n",
    "left_hr <- HR$left[-training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         left_hr\n",
       "pred.glm2    0    1\n",
       "     Down 2668  550\n",
       "     Up    210  322"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Logistic regression\n",
    "fit.model2 <- glm(left ~ satisfaction_level + last_evaluation + number_project + average_montly_hours + time_spend_company + Work_accident + promotion_last_5years + sales + salary, data = HR, family = binomial, subset = training_data)\n",
    "probs2 <- predict(fit.model2, test, type = \"response\")\n",
    "pred.glm2 <- rep(\"Down\", length(probs2))\n",
    "pred.glm2[probs2 > 0.5] <- \"Up\"\n",
    "table(pred.glm2, left_hr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Prediction rate is (2668 + 322) / (2668+550+210+322) = 0.7973 or 80% .In other words, 20% is the training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'gbm' was built under R version 3.4.4\"Loading required package: survival\n",
      "Loading required package: lattice\n",
      "Loading required package: splines\n",
      "Loading required package: parallel\n",
      "Loaded gbm 2.1.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         left_hr\n",
       "pred.gbm2    0    1\n",
       "     Down 2854  651\n",
       "     Up     24  221"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Boosting\n",
    "library(gbm)\n",
    "boost.fit <- gbm(left ~ ., data = train, distribution = \"bernoulli\", n.trees = 5000)\n",
    "boost.probs <- predict(boost.fit, newdata = test, n.trees = 5000)\n",
    "pred.gbm2 <- rep(\"Down\", length(boost.probs))\n",
    "pred.gbm2[boost.probs > 0.5] <- \"Up\"\n",
    "table(pred.gbm2, left_hr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Prediction rate is (2854 + 221) / (2854+651+24+221) = 0.82 or 82% .In other words, 18% is the training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       left_hr\n",
       "pred.mt    0    1\n",
       "   Down 2842   78\n",
       "   Up     36  794"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Bagging\n",
    "library(ipred)\n",
    "mt <- bagging(left ~., data=train, coob=TRUE)\n",
    "mt_probs <- predict(mt, newdata = test)\n",
    "pred.mt <- rep(\"Down\", length(mt_probs))\n",
    "pred.mt[mt_probs > 0.5] <- \"Up\"\n",
    "table(pred.mt, left_hr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Prediction rate is (2842 + 794) / (2842+78+36+794) = 0.9696 or 96% .In other words, 4% is the training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'randomForest' was built under R version 3.4.4\"randomForest 4.6-14\n",
      "Type rfNews() to see new features/changes/bug fixes.\n"
     ]
    }
   ],
   "source": [
    "#Randomforest\n",
    "library(randomForest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in randomForest.default(m, y, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\""
     ]
    }
   ],
   "source": [
    "rf_fit <- randomForest(left ~ . , data = train, mtry = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       left_hr\n",
       "pred.rf    0    1\n",
       "   Down 2833   25\n",
       "   Up      5  887"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_probs <- predict(rf_fit, newdata = test)\n",
    "pred.rf <- rep(\"Down\", length(rf_probs))\n",
    "pred.rf[rf_probs > 0.5] <- \"Up\"\n",
    "table(pred.rf, left_hr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Prediction rate is (2833 + 887) / (2833+25+5+887) = 0.992 or 99% .In other words, 1% is the training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When compared to logistic regression bagging, boosting and randomforest methods are better in accuracy and randomforest with an accuracy of 99% gives the best performance among all these methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
